{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**RAG SYSTEM**"
      ],
      "metadata": {
        "id": "qVv2ezI3BQXi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "7VxWFExH7a2a"
      },
      "outputs": [],
      "source": [
        "# Importing the necessary libraries\n",
        "import openai\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "client = openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Accessing the openai api_key\n",
        "client.api_key = \"API_Key\""
      ],
      "metadata": {
        "id": "nRq_dYdY77OR"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building RAG System"
      ],
      "metadata": {
        "id": "JIPAps419NuX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate embeddings\n",
        "def get_embedding(text, model=\"text-embedding-ada-002\"):\n",
        "    text = text.replace(\"\\n\", \" \")\n",
        "    response = client.Embedding.create(input=text, model=model)\n",
        "    return response['data'][0]['embedding']\n",
        "\n",
        "\n",
        "# Function to calculate similarity (dot product)\n",
        "def calculate_similarity(query_embedding, db_embeddings):\n",
        "    similarities = np.dot(db_embeddings, query_embedding)\n",
        "    return similarities\n",
        "\n",
        "\n",
        "# Retrieving system function to find and retrieve the top 4 results\n",
        "def query_system(question, df, model=\"text-embedding-ada-002\"):\n",
        "    # Generate embedding for the query\n",
        "    query_embedding = get_embedding(question, model=model)\n",
        "\n",
        "    # Calculate similarities\n",
        "    db_embeddings = np.vstack(df['embeddings'])  # Convert list of embeddings to matrix\n",
        "    similarities = calculate_similarity(query_embedding, db_embeddings)\n",
        "\n",
        "    # Retrieve indices of the top 4 most similar rows\n",
        "    top_4_indices = np.argsort(similarities)[-4:][::-1]  # Sort, get last 4, and reverse to descending order\n",
        "    retrieved_rows = df.iloc[top_4_indices]\n",
        "\n",
        "    return retrieved_rows\n",
        "\n",
        "\n",
        "# Response Generation system function using GPT-3.5 Turbo\n",
        "def generate_response(question, retrieved_rows):\n",
        "    # Combine the top 4 contexts into a single context string\n",
        "    combined_context = \"\\n\\n\".join(retrieved_rows['detailed_explanation'].dropna())\n",
        "\n",
        "    # Check if the combined context is valid (not empty)\n",
        "    if not combined_context.strip():\n",
        "        # If no valid context is found, return a direct response without querying GPT\n",
        "        return \"No such code for the occupation.\"\n",
        "\n",
        "    # Use GPT-3.5 Turbo to generate a response based on the combined context\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": (\n",
        "                    \"You are a helpful assistant trained to classify job descriptions and provide occupation codes based on context. \"\n",
        "                    \"Always respond with the occupational code only, without any additional text or explanation. \"\n",
        "                    \"If the context does not match the query, respond with 'NULL'.\"\n",
        "                ),\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"Based on these contexts:\\n\\n{combined_context}\\n\\nAnswer the query: {question}\"\n",
        "            }\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return response['choices'][0]['message']['content']\n"
      ],
      "metadata": {
        "id": "C96aWrOs8CAT"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing the RAG System"
      ],
      "metadata": {
        "id": "2g2AfS8SAZoW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing embeddings data\n",
        "embeddings_data = pd.read_pickle(\"isco_08_df_with_embeddings.pkl\")"
      ],
      "metadata": {
        "id": "ZYG4FWpEAe60"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"This person works as a waiter at restaurant?\"\n",
        "\n",
        "# Retrieve the most relevant row\n",
        "retrieved_row = query_system(query, embeddings_data)\n",
        "\n",
        "# Generate the response\n",
        "response = generate_response(query, retrieved_row)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vmh6p54e9pbA",
        "outputId": "0b72b362-f778-4493-b8f4-d8f185753dea"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5131\n"
          ]
        }
      ]
    }
  ]
}